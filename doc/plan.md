# ğŸ“š æœŸæœ«ä½œä¸šå®Œæ•´è§„åˆ’æŒ‡å—

æ ¹æ®å®é™…ç»éªŒï¼ˆå¤„ç†æ•°æ® + å‘ç°Pandasæ…¢ + æƒ³ç”¨å¹¶è¡Œè®¡ç®—ï¼‰**ä¸€æ­¥æ­¥è§„åˆ’è¿™ä¸ªä½œä¸š**ã€‚

---

## ä¸€ã€é¡¹ç›®é€‰é¢˜ï¼šåŸºäºä½ ç°æœ‰å·¥ä½œçš„æœ€ä½³é€‰æ‹©

### ğŸ¯ æ¨èé¢˜ç›®

> **"åŸºäºæ··åˆå¹¶è¡Œæ¶æ„çš„å¤§è§„æ¨¡é”€å”®æ•°æ®å¤„ç†ä¸åˆ†æç³»ç»Ÿ"**

---

## äºŒã€ä½œä¸šç»“æ„è§„åˆ’ï¼ˆé€éƒ¨åˆ†è¯¦è§£ï¼‰

```mermaid
graph TD
    subgraph "é¡¹ç›®ç»“æ„"
        A[1. é—®é¢˜å®šä¹‰] --> B[2. é¡ºåºåŸºçº¿]
        B --> C[3. å¹¶è¡Œè®¡ç®—]
        C --> D[4. åˆ†å¸ƒå¼è®¡ç®—]
        D --> E[5. æ€§èƒ½åˆ†æ]
        E --> F[6. æ¶æ„å›¾]
        F --> G[7. æ¼”ç¤ºå‡†å¤‡]
    end
```

---

## ä¸‰ã€Part 1ï¼šé—®é¢˜å®šä¹‰ï¼ˆ2-3é¡µPPTï¼‰

### 3.1 ä¸šåŠ¡é—®é¢˜æè¿°

```python
"""
åœºæ™¯ï¼šAå…¬å¸éœ€è¦åˆ†æ2024å¹´XXXé—¨åº—çš„é”€å”®æ•°æ®
æ•°æ®ï¼š5ä¸ªExcelæ–‡ä»¶ï¼Œæ¯ä¸ªçº¦20MBï¼Œæ€»è®¡~100MB
éœ€æ±‚ï¼šè®¡ç®—æ¯ä¸ªäº§å“åœ¨æ¯ä¸ªé—¨åº—çš„å‘¨åº¦é”€é‡ã€é”€å”®é¢ã€åŒæ¯”å˜åŒ–
æ—¶é—´è¦æ±‚ï¼šåŸæœ¬éœ€è¦3å°æ—¶ï¼Œä¸šåŠ¡è¦æ±‚30åˆ†é’Ÿå†…å‡ºç»“æœ
"""
```

### 3.2 è®¡ç®—é—®é¢˜å®šä¹‰

| ç»´åº¦               | è¯´æ˜                                 |
| ------------------ | ------------------------------------ |
| **è¾“å…¥**           | 5ä¸ªExcelæ–‡ä»¶ï¼Œæ¯ä¸ª ~20MB             |
| **è®¡ç®—**           | groupby product_id + store_id + week |
| **è¾“å‡º**           | èšåˆåçš„DataFrameï¼Œ~500ä¸‡è¡Œ          |
| **ç“¶é¢ˆ**           | I/Oï¼ˆè¯»Excelï¼‰+ CPUï¼ˆèšåˆï¼‰          |
| **ä¸ºä»€ä¹ˆéœ€è¦å¹¶è¡Œ** | å•æœº3å°æ—¶ï¼Œæ— æ³•æ»¡è¶³ä¸šåŠ¡éœ€æ±‚          |

### 3.3 é¢„æœŸæ•°æ®è§„æ¨¡

```python
# æ•°æ®è§„æ¨¡ä¼°ç®—
total_files = 5  # ä¸€ä¸ªæœˆ5å‘¨
file_size_mb = 20
total_data_gb = (total_files * file_size_mb) / 1024  # ~10GB

rows_per_file = 500_000
total_rows = total_files * rows_per_file  # 26,000,000è¡Œ

print(f"""
æ•°æ®è§„æ¨¡:
- æ–‡ä»¶æ•°: {total_files}
- æ€»æ•°æ®é‡: {total_data_gb:.1f}GB
- æ€»è¡Œæ•°: {total_rows:,}
- è®¡ç®—å¤æ‚åº¦: O(n log n) èšåˆ
""")
```

---

## å››ã€Part 2ï¼šé¡ºåºåŸºçº¿ï¼ˆ3-4é¡µPPT + ä»£ç ï¼‰

### 4.1 é¡ºåºå®ç°ä»£ç 

```python
# baseline_sequential.py
"""
é¡ºåºåŸºçº¿ç‰ˆæœ¬ - ä½œä¸ºæ€§èƒ½å¯¹æ¯”çš„åŸºå‡†
"""
import pandas as pd
import time
from pathlib import Path

class SequentialProcessor:
    """é¡ºåºå¤„ç†ï¼ˆå•çº¿ç¨‹ï¼‰"""

    def __init__(self, data_dir="/data/raw"):
        self.data_dir = Path(data_dir)
        self.stats = {}

    def process_all(self):
        """é¡ºåºå¤„ç†æ‰€æœ‰æ–‡ä»¶"""
        start_total = time.time()

        all_dfs = []

        # 1. é¡ºåºè¯»å–æ‰€æœ‰Excel
        start_read = time.time()
        files = sorted(self.data_dir.glob("*.xlsx"))

        for i, f in enumerate(files):
            df = pd.read_excel(f)
            df['source_week'] = f.stem
            all_dfs.append(df)
            print(f"è¯»å– {i+1}/{len(files)}: {f.name}")

        read_time = time.time() - start_read
        print(f"è¯»å–å®Œæˆ: {read_time:.2f}ç§’")

        # 2. åˆå¹¶æ‰€æœ‰æ•°æ®
        start_merge = time.time()
        df_combined = pd.concat(all_dfs, ignore_index=True)
        merge_time = time.time() - start_merge

        # 3. èšåˆè®¡ç®—ï¼ˆCPUå¯†é›†å‹ï¼‰
        start_agg = time.time()
        result = df_combined.groupby(['product_id', 'store_id']).agg({
            'quantity': 'sum',
            'revenue': 'sum',
            'price': 'mean'
        }).reset_index()
        agg_time = time.time() - start_agg

        # 4. ä¿å­˜ç»“æœ
        start_save = time.time()
        result.to_csv('sequential_output.csv', index=False)
        save_time = time.time() - start_save

        total_time = time.time() - start_total

        # è®°å½•æ€§èƒ½æ•°æ®
        self.stats = {
            'read_time': read_time,
            'merge_time': merge_time,
            'agg_time': agg_time,
            'save_time': save_time,
            'total_time': total_time,
            'rows_processed': len(df_combined),
            'result_rows': len(result)
        }

        self.print_stats()
        return result

    def print_stats(self):
        """æ‰“å°æ€§èƒ½ç»Ÿè®¡"""
        print("\n" + "="*50)
        print("é¡ºåºåŸºçº¿æ€§èƒ½ç»Ÿè®¡")
        print("="*50)
        for k, v in self.stats.items():
            if 'time' in k:
                print(f"{k}: {v:.2f}ç§’")
            else:
                print(f"{k}: {v}")

        print(f"\næ€»æ—¶é—´: {self.stats['total_time']:.2f}ç§’ = {self.stats['total_time']/60:.2f}åˆ†é’Ÿ")


# è¿è¡Œ
if __name__ == "__main__":
    processor = SequentialProcessor("/data/raw/edeka/2024")
    result = processor.process_all()
```

### 4.2 ç“¶é¢ˆåˆ†æ

```python
# ç“¶é¢ˆåˆ†æä»£ç 
def analyze_bottleneck(stats):
    """åˆ†æè®¡ç®—ç“¶é¢ˆ"""

    total = stats['total_time']

    bottlenecks = {
        'read': (stats['read_time']/total, 'I/Oç“¶é¢ˆ - ç£ç›˜è¯»å–æ…¢'),
        'agg': (stats['agg_time']/total, 'CPUç“¶é¢ˆ - èšåˆè®¡ç®—'),
        'save': (stats['save_time']/total, 'I/Oç“¶é¢ˆ - ç£ç›˜å†™å…¥')
    }

    print("\nç“¶é¢ˆåˆ†æ:")
    for name, (pct, desc) in bottlenecks.items():
        print(f"  {name}: {pct*100:.1f}% - {desc}")

    # æ‰¾å‡ºæœ€å¤§ç“¶é¢ˆ
    max_bottleneck = max(bottlenecks.items(), key=lambda x: x[1][0])
    print(f"\nğŸ”´ æœ€å¤§ç“¶é¢ˆ: {max_bottleneck[0]} ({max_bottleneck[1][1]})")

    return bottlenecks
```

---

## äº”ã€Part 3ï¼šå¹¶è¡Œè®¡ç®—ï¼ˆ4-5é¡µPPT + ä»£ç ï¼‰

### 5.1 å¹¶è¡Œæ¶æ„è®¾è®¡

```mermaid
graph TD
    subgraph "ä¸»è¿›ç¨‹"
        A[æ–‡ä»¶åˆ—è¡¨] --> B[ä»»åŠ¡åˆ†é…]
        B --> C[ç»“æœæ”¶é›†]
        C --> D[æœ€ç»ˆèšåˆ]
    end

    subgraph "Worker 1"
        E[è¯»æ–‡ä»¶1] --> F[å¤„ç†]
        F --> G[ä¸­é—´ç»“æœ1]
    end

    subgraph "Worker 2"
        H[è¯»æ–‡ä»¶2] --> I[å¤„ç†]
        I --> J[ä¸­é—´ç»“æœ2]
    end

    subgraph "Worker N"
        K[è¯»æ–‡ä»¶N] --> L[å¤„ç†]
        L --> M[ä¸­é—´ç»“æœN]
    end

    B --> E
    B --> H
    B --> K

    G --> C
    J --> C
    M --> C
```

### 5.2 å¹¶è¡Œå®ç°ä»£ç 

```python
# parallel_processor.py
"""
å¹¶è¡Œç‰ˆæœ¬ - ä½¿ç”¨ProcessPoolExecutor
"""
import pandas as pd
import numpy as np
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import time
from pathlib import Path
import multiprocessing as mp

class ParallelProcessor:
    """
    å¹¶è¡Œå¤„ç† - åŒæ—¶åˆ©ç”¨å¤šæ ¸CPU
    """

    def __init__(self, data_dir="/data/raw", n_workers=None):
        self.data_dir = Path(data_dir)
        self.n_workers = n_workers or mp.cpu_count()
        self.stats = {}

    def process_one_file(self, file_path):
        """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆæ¯ä¸ªworkeræ‰§è¡Œï¼‰"""
        try:
            # è¯»æ–‡ä»¶
            df = pd.read_excel(file_path)

            # æ·»åŠ æ–‡ä»¶åä¿¡æ¯
            df['source_week'] = Path(file_path).stem

            # æ–‡ä»¶å†…çš„èšåˆï¼ˆå‡å°‘æ•°æ®ä¼ è¾“ï¼‰
            result = df.groupby(['product_id', 'store_id']).agg({
                'quantity': 'sum',
                'revenue': 'sum'
            }).reset_index()

            return result
        except Exception as e:
            print(f"å¤„ç†å¤±è´¥ {file_path}: {e}")
            return None

    def process_parallel(self):
        """å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡ä»¶"""
        start_total = time.time()

        # è·å–æ–‡ä»¶åˆ—è¡¨
        files = list(sorted(self.data_dir.glob("*.xlsx")))
        print(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ {self.n_workers} ä¸ªworker")

        # å¹¶è¡Œè¯»å–å’Œå¤„ç†
        start_process = time.time()

        with ProcessPoolExecutor(max_workers=self.n_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = [executor.submit(self.process_one_file, f) for f in files]

            # æ”¶é›†ç»“æœ
            results = []
            for i, future in enumerate(futures):
                res = future.result()
                if res is not None:
                    results.append(res)
                print(f"å®Œæˆ {i+1}/{len(files)}")

        process_time = time.time() - start_process
        print(f"å¹¶è¡Œå¤„ç†å®Œæˆ: {process_time:.2f}ç§’")

        # åˆå¹¶ç»“æœ
        start_merge = time.time()
        df_combined = pd.concat(results, ignore_index=True)
        merge_time = time.time() - start_merge

        # æœ€ç»ˆèšåˆï¼ˆéœ€è¦è·¨æ–‡ä»¶èšåˆï¼‰
        start_final_agg = time.time()
        final_result = df_combined.groupby(['product_id', 'store_id']).agg({
            'quantity': 'sum',
            'revenue': 'sum'
        }).reset_index()
        final_agg_time = time.time() - start_final_agg

        # ä¿å­˜
        start_save = time.time()
        final_result.to_parquet('parallel_output.parquet', compression='zstd')
        save_time = time.time() - start_save

        total_time = time.time() - start_total

        self.stats = {
            'process_time': process_time,
            'merge_time': merge_time,
            'final_agg_time': final_agg_time,
            'save_time': save_time,
            'total_time': total_time,
            'n_workers': self.n_workers
        }

        return final_result

    def benchmark_vs_sequential(self, seq_time):
        """å¯¹æ¯”é¡ºåºç‰ˆæœ¬"""
        print("\n" + "="*50)
        print("å¹¶è¡Œ vs é¡ºåº æ€§èƒ½å¯¹æ¯”")
        print("="*50)

        par_time = self.stats['total_time']
        speedup = seq_time / par_time
        efficiency = speedup / self.n_workers

        print(f"é¡ºåºæ—¶é—´: {seq_time:.2f}ç§’")
        print(f"å¹¶è¡Œæ—¶é—´: {par_time:.2f}ç§’")
        print(f"åŠ é€Ÿæ¯”: {speedup:.2f}x")
        print(f"å¹¶è¡Œæ•ˆç‡: {efficiency*100:.1f}%")

        return {
            'speedup': speedup,
            'efficiency': efficiency
        }
```

---

## å…­ã€Part 4ï¼šåˆ†å¸ƒå¼è®¡ç®—ï¼ˆ5-6é¡µPPT + ä»£ç ï¼‰

### 6.1 ä¸ºä»€ä¹ˆé€‰Sparkï¼Ÿ

| éœ€æ±‚               | Sparkçš„ä¼˜åŠ¿       |
| ------------------ | ----------------- |
| æ•°æ®é‡å¤§ï¼ˆ>100GBï¼‰ | åˆ†å¸ƒå¼å­˜å‚¨ + è®¡ç®— |
| éœ€è¦å®¹é”™           | è‡ªåŠ¨é‡è¯•          |
| éœ€è¦SQLæ¥å£        | Spark SQL         |
| éœ€è¦ä¸Pythoné›†æˆ   | PySpark           |

### 6.2 æ¶æ„è®¾è®¡

```mermaid
graph TD
    subgraph "æ•°æ®æº"
        A[HDFS/æœ¬åœ°æ–‡ä»¶] --> B[Sparkè¯»å–]
    end

    subgraph "Sparké›†ç¾¤"
        C[MasterèŠ‚ç‚¹]
        D[Worker 1]
        E[Worker 2]
        F[Worker N]
    end

    subgraph "å¤„ç†æµç¨‹"
        G[RDD/DataFrame]
        H[Shuffleèšåˆ]
        I[ç»“æœæ”¶é›†]
    end

    B --> C
    C --> D & E & F
    D & E & F --> G
    G --> H
    H --> I
```

### 6.3 Sparkå®ç°ä»£ç 

```python
# distributed_spark.py
"""
åˆ†å¸ƒå¼ç‰ˆæœ¬ - ä½¿ç”¨PySpark
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import sum, avg, count, col
import time
import pandas as pd

class SparkDistributedProcessor:
    """
    Sparkåˆ†å¸ƒå¼å¤„ç†
    """

    def __init__(self, app_name="SalesAnalytics", master="local[*]"):
        self.spark = SparkSession.builder \
            .appName(app_name) \
            .master(master) \
            .config("spark.sql.shuffle.partitions", "200") \
            .config("spark.executor.memory", "4g") \
            .getOrCreate()

        self.stats = {}

    def process_from_hdfs(self, data_path="hdfs:///data/sales"):
        """
        ä»HDFSè¯»å–å¹¶å¤„ç†
        """
        start_total = time.time()

        # 1. è¯»å–æ•°æ®ï¼ˆåˆ†å¸ƒå¼ï¼‰
        start_read = time.time()
        df = self.spark.read.option("header", "true") \
            .option("inferSchema", "true") \
            .csv(data_path)

        read_time = time.time() - start_read
        print(f"åˆ†å¸ƒå¼è¯»å–å®Œæˆ: {read_time:.2f}ç§’")

        # 2. æ•°æ®æ¸…æ´—ï¼ˆåˆ†å¸ƒå¼ï¼‰
        start_clean = time.time()
        df_clean = df.filter(col("quantity") > 0) \
            .filter(col("revenue") > 0)
        clean_time = time.time() - start_clean

        # 3. èšåˆè®¡ç®—ï¼ˆåˆ†å¸ƒå¼Shuffleï¼‰
        start_agg = time.time()
        result = df_clean.groupBy("product_id", "store_id") \
            .agg(
                sum("quantity").alias("total_quantity"),
                sum("revenue").alias("total_revenue"),
                avg("price").alias("avg_price"),
                count("*").alias("transaction_count")
            )

        # è§¦å‘è®¡ç®—
        result_count = result.count()
        agg_time = time.time() - start_agg

        # 4. ä¿å­˜ç»“æœï¼ˆåˆ†å¸ƒå¼ï¼‰
        start_save = time.time()
        result.write.mode("overwrite") \
            .parquet("hdfs:///results/sales_agg")
        save_time = time.time() - start_save

        total_time = time.time() - start_total

        self.stats = {
            'read_time': read_time,
            'clean_time': clean_time,
            'agg_time': agg_time,
            'save_time': save_time,
            'total_time': total_time,
            'result_rows': result_count
        }

        return result

    def scale_test(self, data_sizes=[1, 10, 100]):
        """
        æ‰©å±•æ€§æµ‹è¯•
        """
        results = []

        for size_gb in data_sizes:
            print(f"\næµ‹è¯•æ•°æ®è§„æ¨¡: {size_gb}GB")

            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_path = f"hdfs:///test/data_{size_gb}gb"

            start = time.time()
            df = self.spark.read.parquet(test_path)
            count = df.count()
            read_time = time.time() - start

            results.append({
                'size_gb': size_gb,
                'rows': count,
                'read_time': read_time,
                'throughput_gb_s': size_gb / read_time
            })

        return pd.DataFrame(results)

    def cleanup(self):
        """æ¸…ç†Sparkä¼šè¯"""
        self.spark.stop()
```

---

## ä¸ƒã€Part 5ï¼šæ€§èƒ½åˆ†æï¼ˆ3-4é¡µPPTï¼‰

### 7.1 æ€§èƒ½å¯¹æ¯”ä»£ç 

```python
# performance_analyzer.py
"""
æ€§èƒ½åˆ†æ - å¯¹æ¯”ä¸‰ç§æ–¹æ¡ˆ
"""
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

class PerformanceAnalyzer:
    """
    æ€§èƒ½å¯¹æ¯”åˆ†æ
    """

    def __init__(self):
        self.results = {}

    def collect_results(self, seq_stats, par_stats, spark_stats):
        """æ”¶é›†æ‰€æœ‰ç»“æœ"""
        self.results = {
            'Sequential': seq_stats,
            'Parallel (CPU)': par_stats,
            'Distributed (Spark)': spark_stats
        }

    def plot_time_comparison(self):
        """æ—¶é—´å¯¹æ¯”å›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        # 1. æ€»æ—¶é—´å¯¹æ¯”
        ax1 = axes[0, 0]
        labels = list(self.results.keys())
        times = [self.results[l]['total_time'] for l in labels]

        bars = ax1.bar(labels, times, color=['red', 'green', 'blue'])
        ax1.set_ylabel('æ—¶é—´ (ç§’)')
        ax1.set_title('æ€»æ‰§è¡Œæ—¶é—´å¯¹æ¯”')
        ax1.set_yscale('log')  # å¯¹æ•°åæ ‡

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, t in zip(bars, times):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height()*1.1,
                    f'{t:.1f}s', ha='center', va='bottom')

        # 2. å„é˜¶æ®µæ—¶é—´å¯¹æ¯”
        ax2 = axes[0, 1]
        stages = ['read', 'process', 'agg', 'save']

        x = np.arange(len(stages))
        width = 0.25

        for i, (name, stats) in enumerate(self.results.items()):
            stage_times = [stats.get(f'{s}_time', 0) for s in stages]
            ax2.bar(x + i*width, stage_times, width, label=name)

        ax2.set_xticks(x + width)
        ax2.set_xticklabels(stages)
        ax2.set_ylabel('æ—¶é—´ (ç§’)')
        ax2.set_title('å„é˜¶æ®µè€—æ—¶å¯¹æ¯”')
        ax2.legend()

        # 3. åŠ é€Ÿæ¯”
        ax3 = axes[1, 0]
        seq_time = self.results['Sequential']['total_time']

        speedups = []
        for name in ['Parallel (CPU)', 'Distributed (Spark)']:
            speedups.append(seq_time / self.results[name]['total_time'])

        bars = ax3.bar(['Parallel', 'Distributed'], speedups, color=['green', 'blue'])
        ax3.axhline(y=1, color='red', linestyle='--', label='åŸºçº¿(1x)')
        ax3.set_ylabel('åŠ é€Ÿæ¯” (å€)')
        ax3.set_title('å¹¶è¡Œ/åˆ†å¸ƒå¼åŠ é€Ÿæ¯”')

        for bar, s in zip(bars, speedups):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height()*1.05,
                    f'{s:.2f}x', ha='center')

        # 4. å¯æ‰©å±•æ€§åˆ†æ
        ax4 = axes[1, 1]

        # æ¨¡æ‹Ÿä¸åŒæ•°æ®è§„æ¨¡
        data_sizes = [1, 10, 50, 100, 500]  # GB
        seq_times = [s * 60 for s in data_sizes]  # å‡è®¾çº¿æ€§å¢é•¿
        par_times = [s * 15 for s in data_sizes]  # å‡è®¾8æ ¸åŠ é€Ÿ
        dist_times = [s * 3 for s in data_sizes]  # å‡è®¾é›†ç¾¤åŠ é€Ÿ

        ax4.plot(data_sizes, seq_times, 'r-o', label='Sequential')
        ax4.plot(data_sizes, par_times, 'g-o', label='Parallel')
        ax4.plot(data_sizes, dist_times, 'b-o', label='Distributed')
        ax4.set_xlabel('æ•°æ®è§„æ¨¡ (GB)')
        ax4.set_ylabel('å¤„ç†æ—¶é—´ (ç§’)')
        ax4.set_title('å¯æ‰©å±•æ€§åˆ†æ')
        ax4.legend()
        ax4.set_xscale('log')
        ax4.set_yscale('log')

        plt.tight_layout()
        plt.savefig('performance_analysis.png', dpi=150)
        plt.show()

    def print_summary_table(self):
        """æ‰“å°æ€»ç»“è¡¨æ ¼"""
        print("\n" + "="*80)
        print("æ€§èƒ½å¯¹æ¯”æ€»ç»“")
        print("="*80)

        df = pd.DataFrame(self.results).T
        print(df.round(2))

        # è®¡ç®—åŠ é€Ÿæ¯”
        seq_time = self.results['Sequential']['total_time']
        print("\nåŠ é€Ÿæ¯”:")
        for name in ['Parallel (CPU)', 'Distributed (Spark)']:
            speedup = seq_time / self.results[name]['total_time']
            print(f"  {name}: {speedup:.2f}x")
```

---

## å…«ã€Part 6ï¼šæ¶æ„å›¾ï¼ˆ1é¡µPPTï¼‰

```mermaid
graph TD
    subgraph "æ•°æ®æºå±‚"
        A[Excelæ–‡ä»¶<br/>52ä¸ª/å¹´]
        B[å®æ—¶æ•°æ®æµ<br/>Kafka]
    end

    subgraph "æ•°æ®æ¥å…¥å±‚"
        C[Python ETL]
        D[Spark Streaming]
    end

    subgraph "å­˜å‚¨å±‚"
        E[(SQLite<br/>å…ƒæ•°æ®)]
        F[(Parquet<br/>åˆ—å¼å­˜å‚¨)]
        G[(HDFS<br/>åˆ†å¸ƒå¼å­˜å‚¨)]
    end

    subgraph "è®¡ç®—å±‚"
        H[OpenMP<br/>å¤šæ ¸å¹¶è¡Œ]
        I[Spark<br/>åˆ†å¸ƒå¼è®¡ç®—]
        J[CUDA<br/>GPUåŠ é€Ÿ]
    end

    subgraph "ç»“æœå±‚"
        K[èšåˆç»“æœ]
        L[æ€§èƒ½æŒ‡æ ‡]
        M[å¯è§†åŒ–æŠ¥è¡¨]
    end

    A --> C
    B --> D

    C --> E
    C --> F
    D --> G

    F --> H
    G --> I
    F --> J

    H --> K
    I --> K
    J --> K

    K --> L
    L --> M
```

---

## ä¹ã€Part 7ï¼šæ¼”ç¤ºå‡†å¤‡ï¼ˆ15-20åˆ†é’Ÿï¼‰

### 9.1 æ¼”ç¤ºæµç¨‹

| æ—¶é—´æ®µ        | å†…å®¹     | è¦ç‚¹                             |
| ------------- | -------- | -------------------------------- |
| **0-2åˆ†é’Ÿ**   | é—®é¢˜å®šä¹‰ | å±•ç¤ºæ•°æ®è§„æ¨¡ã€è®¡ç®—éœ€æ±‚ã€ä¸šåŠ¡ä»·å€¼ |
| **2-5åˆ†é’Ÿ**   | é¡ºåºåŸºçº¿ | è·‘ä»£ç ï¼Œå±•ç¤ºç“¶é¢ˆï¼ˆ3å°æ—¶ï¼‰        |
| **5-8åˆ†é’Ÿ**   | å¹¶è¡Œè®¡ç®— | è·‘ä»£ç ï¼Œå±•ç¤ºåŠ é€Ÿï¼ˆ30åˆ†é’Ÿï¼‰       |
| **8-11åˆ†é’Ÿ**  | åˆ†å¸ƒå¼   | å±•ç¤ºæ¶æ„ï¼Œæ¨¡æ‹Ÿè¿è¡Œï¼ˆ5åˆ†é’Ÿï¼‰      |
| **11-13åˆ†é’Ÿ** | æ€§èƒ½å¯¹æ¯” | å±•ç¤ºå›¾è¡¨ï¼Œè§£é‡ŠåŠ é€Ÿæ¯”             |
| **13-15åˆ†é’Ÿ** | æ¶æ„å›¾   | è§£é‡Šç»„ä»¶å…³ç³»                     |
| **15-18åˆ†é’Ÿ** | ä»£ç èµ°è¯» | å…³é”®ä»£ç ç‰‡æ®µè§£é‡Š                 |
| **18-20åˆ†é’Ÿ** | Q&A      | å›ç­”é—®é¢˜                         |

### 9.2 æ¼”ç¤ºå‡†å¤‡æ¸…å•

```markdown
## æ¼”ç¤ºå‡†å¤‡æ¸…å•

### æ•°æ®å‡†å¤‡

- [ ] å‡†å¤‡å¥½52ä¸ªæµ‹è¯•Excelæ–‡ä»¶ï¼ˆå¯ç”¨è„šæœ¬ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
- [ ] å‡†å¤‡å¥½HDFSç¯å¢ƒï¼ˆæˆ–æœ¬åœ°æ¨¡æ‹Ÿï¼‰
- [ ] å‡†å¤‡å¥½SQLiteæ•°æ®åº“

### ä»£ç å‡†å¤‡

- [ ] é¡ºåºç‰ˆæœ¬ï¼ˆbaseline_sequential.pyï¼‰
- [ ] å¹¶è¡Œç‰ˆæœ¬ï¼ˆparallel_processor.pyï¼‰
- [ ] Sparkç‰ˆæœ¬ï¼ˆdistributed_spark.pyï¼‰
- [ ] æ€§èƒ½åˆ†æè„šæœ¬ï¼ˆperformance_analyzer.pyï¼‰

### PPTå‡†å¤‡

- [ ] é—®é¢˜å®šä¹‰ï¼ˆ2-3é¡µï¼‰
- [ ] æ¶æ„å›¾ï¼ˆ1é¡µï¼‰
- [ ] ä»£ç ç‰‡æ®µï¼ˆ2-3é¡µï¼‰
- [ ] æ€§èƒ½å›¾è¡¨ï¼ˆ2-3é¡µï¼‰
- [ ] æ€»ç»“ï¼ˆ1é¡µï¼‰

### ç¯å¢ƒæ£€æŸ¥

- [ ] Pythonç¯å¢ƒ + ä¾èµ–
- [ ] Sparkç¯å¢ƒ
- [ ] æµ‹è¯•æ•°æ®å¯è¿è¡Œ
- [ ] æŠ•å½±ä»ªå…¼å®¹
```

---

## åã€è¯„åˆ†æ ‡å‡†å¯¹åº”è¡¨

| ä½œä¸šè¦æ±‚   | å®ç°                                    | é¡µç /ä½ç½® |
| ---------- | --------------------------------------- | --------- |
| é—®é¢˜å®šä¹‰   | 10GBæ•°æ®ï¼Œ52ä¸ªæ–‡ä»¶ï¼Œ3å°æ—¶â†’30åˆ†é’Ÿ        | Part 1    |
| é¡ºåºåŸºçº¿   | SequentialProcessorç±»                   | Part 2    |
| ç“¶é¢ˆåˆ†æ   | analyze_bottleneck()                    | Part 2.4  |
| å¹¶è¡Œè®¡ç®—   | ParallelProcessor + ProcessPoolExecutor | Part 3    |
| åˆ†å¸ƒå¼è®¡ç®— | SparkDistributedProcessor               | Part 4    |
| æ¶æ„å›¾     | åŒ…å«æ•°æ®æµ+è®¡ç®—å±‚çš„Mermaidå›¾            | Part 6    |
| æ€§èƒ½åˆ†æ   | å¯¹æ¯”å›¾è¡¨ + åŠ é€Ÿæ¯”è®¡ç®—                   | Part 5    |
| ä»£ç èµ°è¯»   | å…³é”®ä»£ç ç‰‡æ®µè§£é‡Š                        | Part 7    |

---

## åä¸€ã€æ—¶é—´è§„åˆ’å»ºè®®

| æ—¶é—´          | ä»»åŠ¡                      |
| ------------- | ------------------------- |
| **Day 1-2**   | å†™é¡ºåºåŸºçº¿ï¼Œæµ‹å‡ºåŸºå‡†æ—¶é—´  |
| **Day 3-4**   | å®ç°å¹¶è¡Œç‰ˆæœ¬ï¼Œè°ƒä¼˜        |
| **Day 5-6**   | æ­å»ºSparkç¯å¢ƒï¼Œå®ç°åˆ†å¸ƒå¼ |
| **Day 7-8**   | è·‘æ€§èƒ½æµ‹è¯•ï¼Œæ”¶é›†æ•°æ®      |
| **Day 9-10**  | åšPPTï¼Œç”»æ¶æ„å›¾           |
| **Day 11-12** | æ’ç»ƒæ¼”ç¤ºï¼Œå‡†å¤‡Q&A         |
| **Day 13**    | æœ€ç»ˆæ£€æŸ¥ï¼Œæäº¤            |

---

## åäºŒã€ä¸€å¥è¯æ€»ç»“

> **æœŸæœ«ä½œä¸š = æŠŠä½ ç°åœ¨åšçš„KÃ¼hneæ•°æ®å¤„ç†ï¼Œç”¨é¡ºåº(æ…¢)ã€å¹¶è¡Œ(å¿«)ã€åˆ†å¸ƒå¼(æ›´å¿«)ä¸‰ç§æ–¹å¼å®ç°ï¼Œç„¶åå¯¹æ¯”è¯æ˜â€”â€”è¿™å°±æ˜¯æ•™æˆè¦çš„ï¼**

éœ€è¦æˆ‘å¸®ä½ ç»†åŒ–æŸä¸€éƒ¨åˆ†çš„ä»£ç æˆ–PPTå†…å®¹å—ï¼Ÿ
